{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    "Referencing https://becominghuman.ai/building-an-image-classifier-using-deep-learning-in-python-totally-from-a-beginners-perspective-be8dbaf22dd8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing the requirements\n",
    "```python\n",
    "pip install theanos\n",
    "pip install keras\n",
    "pip install tensorflow\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import relevant modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Step 1 - Convolution\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))\n",
    "\n",
    "# Step 2 - Poolingu\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adding a second convolutional layer\n",
    "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Step 3 - Flattening\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Step 4 - Full connection\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the CNN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the model using train images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4676 images belonging to 2 classes.\n",
      "Found 2023 images belonging to 2 classes.\n",
      "Epoch 1/25\n",
      "1000/1000 [==============================] - 158s 158ms/step - loss: 0.4963 - acc: 0.7588 - val_loss: 0.8127 - val_acc: 0.6157\n",
      "Epoch 2/25\n",
      "1000/1000 [==============================] - 145s 145ms/step - loss: 0.4033 - acc: 0.8127 - val_loss: 0.6592 - val_acc: 0.7024\n",
      "Epoch 3/25\n",
      "1000/1000 [==============================] - 140s 140ms/step - loss: 0.3521 - acc: 0.8424 - val_loss: 0.7391 - val_acc: 0.6914\n",
      "Epoch 4/25\n",
      "1000/1000 [==============================] - 151s 151ms/step - loss: 0.2920 - acc: 0.8744 - val_loss: 0.7510 - val_acc: 0.7355\n",
      "Epoch 5/25\n",
      "1000/1000 [==============================] - 152s 152ms/step - loss: 0.2396 - acc: 0.8988 - val_loss: 0.9940 - val_acc: 0.7192\n",
      "Epoch 6/25\n",
      "1000/1000 [==============================] - 142s 142ms/step - loss: 0.1786 - acc: 0.9289 - val_loss: 1.5256 - val_acc: 0.6624\n",
      "Epoch 7/25\n",
      "1000/1000 [==============================] - 138s 138ms/step - loss: 0.1406 - acc: 0.9461 - val_loss: 0.9181 - val_acc: 0.7582\n",
      "Epoch 8/25\n",
      "1000/1000 [==============================] - 140s 140ms/step - loss: 0.1101 - acc: 0.9581 - val_loss: 1.0918 - val_acc: 0.7462\n",
      "Epoch 9/25\n",
      "1000/1000 [==============================] - 143s 143ms/step - loss: 0.0849 - acc: 0.9690 - val_loss: 1.1532 - val_acc: 0.7477\n",
      "Epoch 10/25\n",
      "1000/1000 [==============================] - 143s 143ms/step - loss: 0.0720 - acc: 0.9740 - val_loss: 2.0798 - val_acc: 0.6853\n",
      "Epoch 11/25\n",
      "1000/1000 [==============================] - 144s 144ms/step - loss: 0.0594 - acc: 0.9780 - val_loss: 1.3752 - val_acc: 0.7532\n",
      "Epoch 12/25\n",
      "1000/1000 [==============================] - 145s 145ms/step - loss: 0.0538 - acc: 0.9812 - val_loss: 1.7832 - val_acc: 0.7189\n",
      "Epoch 13/25\n",
      "1000/1000 [==============================] - 144s 144ms/step - loss: 0.0468 - acc: 0.9830 - val_loss: 1.5284 - val_acc: 0.7421\n",
      "Epoch 14/25\n",
      "1000/1000 [==============================] - 144s 144ms/step - loss: 0.0415 - acc: 0.9845 - val_loss: 1.9730 - val_acc: 0.7135\n",
      "Epoch 15/25\n",
      "1000/1000 [==============================] - 144s 144ms/step - loss: 0.0404 - acc: 0.9851 - val_loss: 1.4084 - val_acc: 0.7554\n",
      "Epoch 16/25\n",
      "1000/1000 [==============================] - 138s 138ms/step - loss: 0.0409 - acc: 0.9855 - val_loss: 1.7274 - val_acc: 0.7455\n",
      "Epoch 17/25\n",
      "1000/1000 [==============================] - 136s 136ms/step - loss: 0.0344 - acc: 0.9882 - val_loss: 2.0995 - val_acc: 0.7071\n",
      "Epoch 18/25\n",
      "1000/1000 [==============================] - 146s 146ms/step - loss: 0.0294 - acc: 0.9898 - val_loss: 2.2994 - val_acc: 0.6993\n",
      "Epoch 19/25\n",
      "1000/1000 [==============================] - 150s 150ms/step - loss: 0.0323 - acc: 0.9891 - val_loss: 2.0006 - val_acc: 0.7287\n",
      "Epoch 20/25\n",
      "1000/1000 [==============================] - 143s 143ms/step - loss: 0.0314 - acc: 0.9890 - val_loss: 1.7407 - val_acc: 0.7304\n",
      "Epoch 21/25\n",
      "1000/1000 [==============================] - 149s 149ms/step - loss: 0.0253 - acc: 0.9911 - val_loss: 1.8093 - val_acc: 0.7331\n",
      "Epoch 22/25\n",
      "1000/1000 [==============================] - 141s 141ms/step - loss: 0.0322 - acc: 0.9892 - val_loss: 1.5592 - val_acc: 0.7614\n",
      "Epoch 23/25\n",
      "1000/1000 [==============================] - 145s 145ms/step - loss: 0.0205 - acc: 0.9927 - val_loss: 1.9192 - val_acc: 0.7375\n",
      "Epoch 24/25\n",
      "1000/1000 [==============================] - 146s 146ms/step - loss: 0.0241 - acc: 0.9916 - val_loss: 1.7122 - val_acc: 0.7584\n",
      "Epoch 25/25\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0251 - acc: 0.9907 - val_loss: 2.2485 - val_acc: 0.7237\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x430a2f3390>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip = True\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    './CNN_Data/training_set',\n",
    "    target_size = (64, 64),\n",
    "    batch_size = 32,\n",
    "    class_mode = 'binary'\n",
    ")\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "    './CNN_Data/test_set',\n",
    "    target_size = (64, 64),\n",
    "    batch_size = 32,\n",
    "    class_mode = 'binary'\n",
    ")\n",
    "\n",
    "classifier.fit_generator(\n",
    "    training_set,\n",
    "    steps_per_epoch = 1000,\n",
    "    epochs = 25,\n",
    "    validation_data = test_set,\n",
    "    validation_steps = 500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cats': 0, 'dogs': 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image was predicted to belong to cat class. (filename: cat1.jpg)\n",
      "This image was predicted to belong to dog class. (filename: cat2.jpg)\n",
      "This image was predicted to belong to dog class. (filename: cat3.jpg)\n",
      "This image was predicted to belong to dog class. (filename: cat4.jpg)\n",
      "This image was predicted to belong to dog class. (filename: cat5.jpg)\n",
      "This image was predicted to belong to dog class. (filename: dog1.jpg)\n",
      "This image was predicted to belong to dog class. (filename: dog2.png)\n",
      "This image was predicted to belong to cat class. (filename: dog3.jpg)\n",
      "This image was predicted to belong to dog class. (filename: dog4.jpg)\n",
      "This image was predicted to belong to dog class. (filename: dog5.jpg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "predict_root = './predictme/'\n",
    "for filename in os.listdir(predict_root):\n",
    "    \n",
    "    test_image = image.load_img(\n",
    "        predict_root + filename, \n",
    "        target_size = (64, 64)\n",
    "    )\n",
    "\n",
    "    test_image = image.img_to_array(test_image)\n",
    "    test_image = np.expand_dims(test_image, axis = 0)\n",
    "    result = classifier.predict(test_image)\n",
    "\n",
    "    if result[0][0] == 1:\n",
    "        prediction = 'dog'\n",
    "    else:\n",
    "        prediction = 'cat'\n",
    "\n",
    "    print(\"This image was predicted to belong to {} class. (filename: {})\".format(prediction, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
